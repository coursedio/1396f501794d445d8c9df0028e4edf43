WEBVTT

1
00:00:01.000 --> 00:00:02.007
- So now you've seen three examples

2
00:00:02.007 --> 00:00:05.003
of supervised machine-learning algorithms.

3
00:00:05.003 --> 00:00:07.000
There was K nearest neighbor,

4
00:00:07.000 --> 00:00:10.002
regression analysis, and naive bayes.

5
00:00:10.002 --> 00:00:13.003
These are most often used for classifying,

6
00:00:13.003 --> 00:00:15.004
and then there was K means clustering

7
00:00:15.004 --> 00:00:19.002
which is used for unsupervised learning and clustering.

8
00:00:19.002 --> 00:00:22.006
Remember that each of these is like a kitchen tool.

9
00:00:22.006 --> 00:00:25.001
These tools are designed for something specific,

10
00:00:25.001 --> 00:00:28.000
but you can still be creative with how you use them.

11
00:00:28.000 --> 00:00:30.009
It's the same way you can use a fork to whip up eggs

12
00:00:30.009 --> 00:00:33.005
or a knife to pit your avocado.

13
00:00:33.005 --> 00:00:35.006
But as any good chef knows,

14
00:00:35.006 --> 00:00:38.004
you never just present one dish.

15
00:00:38.004 --> 00:00:41.009
Instead you're judged by the whole meal.

16
00:00:41.009 --> 00:00:44.005
That's why it's very common for data science teams

17
00:00:44.005 --> 00:00:47.007
to do something called ensemble modeling.

18
00:00:47.007 --> 00:00:49.006
If you're an actor or music fan,

19
00:00:49.006 --> 00:00:52.002
then you probably have heard the term ensemble.

20
00:00:52.002 --> 00:00:54.009
It's when a group performs together.

21
00:00:54.009 --> 00:00:57.007
It's the same thing with machine-learning algorithms.

22
00:00:57.007 --> 00:01:00.003
There's a few different ways to create ensembles.

23
00:01:00.003 --> 00:01:03.009
The most popular is bagging and stacking.

24
00:01:03.009 --> 00:01:06.001
Bagging is when you use several versions

25
00:01:06.001 --> 00:01:08.006
of the same machine-learning algorithm.

26
00:01:08.006 --> 00:01:09.008
Stacking is when you use

27
00:01:09.008 --> 00:01:12.000
several different machine-learning algorithms,

28
00:01:12.000 --> 00:01:15.002
then you stack them on top of one another.

29
00:01:15.002 --> 00:01:18.001
I used to work for a large home improvement retailer.

30
00:01:18.001 --> 00:01:19.003
One of their challenges

31
00:01:19.003 --> 00:01:22.002
was what items do they put near the checkout?

32
00:01:22.002 --> 00:01:24.006
You'd be surprised how much retailers earn

33
00:01:24.006 --> 00:01:27.009
by selling something just a few minutes before you checkout.

34
00:01:27.009 --> 00:01:29.003
So this was a big challenge

35
00:01:29.003 --> 00:01:30.003
and they wanted to create

36
00:01:30.003 --> 00:01:33.003
an ensemble of machine-learning algorithms.

37
00:01:33.003 --> 00:01:36.009
They debated which ensemble might lead to the best results.

38
00:01:36.009 --> 00:01:38.003
They could use bagging

39
00:01:38.003 --> 00:01:41.000
to try different results of the same algorithm.

40
00:01:41.000 --> 00:01:43.007
Then they'd see if they could improve their accuracy.

41
00:01:43.007 --> 00:01:45.005
This was a national retail chain

42
00:01:45.005 --> 00:01:46.008
so they could pull training data

43
00:01:46.008 --> 00:01:48.009
from stores throughout the country.

44
00:01:48.009 --> 00:01:51.005
So they could get data samples from random stores,

45
00:01:51.005 --> 00:01:53.002
then use K nearest neighbor

46
00:01:53.002 --> 00:01:55.009
to classify those datasets separately.

47
00:01:55.009 --> 00:01:58.005
Then they would aggregate those results together

48
00:01:58.005 --> 00:02:01.002
to see if they could come up with a larger trend.

49
00:02:01.002 --> 00:02:03.003
They would aggregate the insights

50
00:02:03.003 --> 00:02:06.006
of what people purchased right before checkup.

51
00:02:06.006 --> 00:02:09.002
In a sense, they were averaging out the insights

52
00:02:09.002 --> 00:02:12.009
to see if they could come up with a more accurate result.

53
00:02:12.009 --> 00:02:15.007
The retailer could also try boosting.

54
00:02:15.007 --> 00:02:18.003
Here instead of averaging the insights together,

55
00:02:18.003 --> 00:02:21.006
they'd boost the results step by step

56
00:02:21.006 --> 00:02:23.004
so the retailer could take a training set

57
00:02:23.004 --> 00:02:25.005
of their most popular items.

58
00:02:25.005 --> 00:02:28.002
Let's say that their best selling item was a hammer.

59
00:02:28.002 --> 00:02:30.001
Then they could use K nearest neighbor

60
00:02:30.001 --> 00:02:32.007
to see what's often bought with the hammer.

61
00:02:32.007 --> 00:02:35.007
Let's just say it was nails and a tool belt.

62
00:02:35.007 --> 00:02:37.005
Now most of us intuitively know

63
00:02:37.005 --> 00:02:39.000
that if someone buys a hammer,

64
00:02:39.000 --> 00:02:41.000
they're more likely to buy nails.

65
00:02:41.000 --> 00:02:42.002
But that might not help us

66
00:02:42.002 --> 00:02:44.006
if we want to put something near the checkout line.

67
00:02:44.006 --> 00:02:48.003
For that, we might want to use something like naive bayes.

68
00:02:48.003 --> 00:02:50.005
Remember that naive bayes is naive

69
00:02:50.005 --> 00:02:54.004
because it doesn't assume that predictors are correlated.

70
00:02:54.004 --> 00:02:56.006
So we don't assume that if you're buying a hammer,

71
00:02:56.006 --> 00:02:58.002
you're going to need nails.

72
00:02:58.002 --> 00:03:01.007
Instead it will will predict other items that are popular

73
00:03:01.007 --> 00:03:04.000
but might not seem related.

74
00:03:04.000 --> 00:03:05.005
Maybe people who buy hammers

75
00:03:05.005 --> 00:03:08.001
are more likely to buy chocolate bars.

76
00:03:08.001 --> 00:03:10.004
Mixing and matching your machine-learning algorithms

77
00:03:10.004 --> 00:03:13.004
will give you different insights with different results.

78
00:03:13.004 --> 00:03:15.000
Like any good ensemble,

79
00:03:15.000 --> 00:03:16.006
the accuracy of predictions

80
00:03:16.006 --> 00:03:20.002
will depend on the creativity of your data science team.
